{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcTiIZO0Tmxj"
   },
   "source": [
    "# CSE252D: Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6XhgQOxTmxm"
   },
   "source": [
    "## Q1: UNet for Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWOw-zhlTmxn"
   },
   "source": [
    "1. **Check the codes in `Segmentation`.** In this homework, we will provide the dataset loader, the evaluation code, the basic UNet structure and some useful functions. You will be asked to try different variations of network structure and decide the best training strategies to obtain good results. Like in previous homeworks, you are welcome to cite any open source codes that help you improve performance. The provided codes include:\n",
    "    1. `test.py`: The file for evaluation. \n",
    "    2. `dataLoader.py`: The file to load the data for training and testing.  \n",
    "    3. `model.py`: The file for models. The residual block (`ResBlock`) and the code to load pretrained weights of resnet18 (`loadPretrainedWeight`) are given. The basic encoder and decoder are also given as a reference. \n",
    "    4. `colormap.mat`: The color map used to visualize segmentation results. \n",
    "    5. `utils.py`: The file for two useful functions. The `computeAccuracy` function computes the unnormalized confusion matrix of each batch of labels. The `save\\_label` function turns the label into an image using the given color map and saves the image at the assigned location. Also see `test.py` for how these two functions are being used. \n",
    "    6. `train.py`: An empty file where you will implement your training script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pXBu_NzTmxp"
   },
   "source": [
    "2. **Implement the network structures.**  You are required to implement 2 versions of UNet structure since the basic structure has already been given. In all three versions,  the `resnet18` structure before average pooling and fully connected layer will be used as the building block for encoder. You are strongly recommended to use weights pretrained on ImageNet, which may have a major impact on the performance. \n",
    "    1. `Basic UNet`: The code is given as a reference. Please see `encoder` and `decoder` class in `model.py`. The `encoder` comes from `resnet18` and the decoder consists of transpose convolutional layers and bilinear interpolation layers so that the final output will be of the same size as the image. Skip links are added to help the network recover more details. Please do not change the encoder. However, you are free to change the decoder, while ensuring that the structure of your decoder across three versions of the networks are similar so that you can make a fair comparison of their performances. \n",
    "    2. `UNet with dilation`: We modify the encoder to a dilated `resnet18` as described in Section 2 of [1] (You are not required to consider degridding in Section 4 of [1] ). We set the stride of the last 4 residual blocks to be 1 so that the highest level feature maps will be $4\\times 4$ times larger. To increase the receptive field, we set the dilation of residual blocks that are fourth and third from the end to be 2, while the dilation of the residual blocks that are first and second from the end are set to 4.  The decoder should be modified accordingly. Implement your new encoder and decoder under class `encoderDilation` and `decoderDilation`. Ensure that for images of arbitrary shapes, the decoder will give segmentation outputs of the same shape.  **[15 points]**\n",
    "    3. `UNet with dilation and pyramid pooling`:  Based on the encoder-decoder structure with dilation, add pyramid pooling layer after the last residual block of encoder.  Implement the pyramid pooling layer following [2]. Notice that after adding the pyramid layer, the number of channels of the output feature to the first transpose convolutional layer will change from 512 to 1024. Please implement your new encoder and decoder under classes `encoderSPP` and `decoderSPP`, respectively.  **[15 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q1.2.B here, Paste your code here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q1.2.C here, Paste your code here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TavmhnHrTmxr"
   },
   "source": [
    "3. **Implement training script and train the network.** Train your network using 1464 images from the training set of PASCAL VOC 2012. The dataset is on the server `/datasets/cs252d-sp22-a00-public/hw3_data/VOCdevkit`. If you are not familiar with training scripts, you can refer to `test.py`  in this homework and `casia_train.py` in the previous homework. The structures of the training script are very similar. Please remember to output the training loss and training accuracy which may help you find the best hyper parameters.  **[40 points]**\n",
    "    1. To accelerate the training speed, you can use the Pytorch multi-threaded data loader. **Important:** if you use multi-threaded data loader, remember to either randomly shuffle the data or change the random seeds after every epoch. Otherwise you will have severe overfitting issues because the data loader will always crop the same region of the image. \n",
    "    2. It is recommended to compute the prediction mIoU every epoch, since the curve of mIoU can be very different from the inverse of loss function. It may help you find the best training strategy. \n",
    "    3. To overcome over-fitting issues, you are encouraged to adopt more aggressive data augmentation methods, such as flipping the images or changing the intensity. \n",
    "    4. There are many things that may influence performance, such as learning rate, batch size and network structure of encoder and decoder. It might be hard to achieve state-of-the-art results. **The grading of the homework will not focus on the final mean IoU but more on analysis.** So don't be too worried if you cannot get very good performance. Just make sure that you describe what you observe and answer the questions succinctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Point out where you implement your training script here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UEWIr3CTmxs"
   },
   "source": [
    "4. **Answer the following questions:**\n",
    "    1. Describe the loss function you use to train the semantic segmentation network. If you change the structure of the decoder, describe your new network architecture. **[10 points]**\n",
    "    2. Describe your training details, such as: what kind of optimizer is used to train the network, what's the learning rate and the batch size, whether you decrease the learning rate as the training progresses, number of epochs required to train the network, or any other details that you find important for  performance. Note that in order to compare the three network structures, learning details for them should be the same. **[10 points]**\n",
    "    3. Draw the loss curves of the three network structures in the same graph.  **[10 points]**\n",
    "    4. Evaluate the trained models using the following commands. Draw a table to summarize quantitative performances of the 3 variations of the UNet structure. The table should include the IoU for each of the 21 categories of objects and the mean IoU across all categories. **[10 points]** :\n",
    "        1. `Basic UNet`: `python test.py`. The testing mean IoU of 21 categories of object are stored in  `test/accuracy.npy`. You can add flags if necessary. \n",
    "        2. `UNet with dilation`: `python test.py --isDilation`. The testing mean IoU of 21 categories of objects are stored in `test_dilation/accuracy.npy`. You can add flags if necessary. \n",
    "        3. `UNet with dilation and pyramid pooling`: `python test.py --isSpp`. The testing mean IoU of 21 categories of object are stored in  `test_spp/accuracy.npy`. You can add flags if necessary. \n",
    "    5. Make a figure for qualitative comparisons of the 3 methods, shown on 4 different input images. Please show the segmentation results for the same image but different networks so the differences can be compared. Briefly describe the results you obtain and any observations. **[10 points]** \n",
    "    6. Explain your observations in terms of: (i) what choices helped improve the accuracy and (ii) what other steps could have been tried to further improve accuracy?  **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62UvcpXaTmxt"
   },
   "source": [
    "``Answer Q1.4.A here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3oP-cxDTmxu"
   },
   "source": [
    "``Answer Q1.4.B here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sTXgPQXTmxv"
   },
   "source": [
    "``Answer Q1.4.C here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPjDADkLTmxx"
   },
   "source": [
    "``Answer Q1.4.D here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRNqqX7pTmxy"
   },
   "source": [
    "``Answer Q1.4.E here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhfwRrPGTmx0"
   },
   "source": [
    "``Answer Q1.4.F here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peyFOlNWTmx1"
   },
   "source": [
    "## Q2: SSD [3] Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsyOKUaxTmx3"
   },
   "source": [
    "1. **Check the codes in `Detection`.** The codes are modified from ``https://github.com/amdegroot/ssd.pytorch``. Run `eval.py` code to get the object detection average precision (AP) on the PASCAL VOC 2012 dataset. The model is already trained on the PASCAL VOC 2012 object detection dataset and stored at ``/datasets/cs252d-sp22-a00-public/hw3_data/detection/weights/VOC.pth``. Draw a table in your report summarizing the AP of all 20 object categories and their mean.   **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9nMMODaTmx4"
   },
   "source": [
    "``Answer Q2.1 here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jm-1Kis8Tmx4"
   },
   "source": [
    "2. **Answer the following questions:**\n",
    "    1.  Briefly explain how average precision is computed for PASCAL VOC 2012 dataset. Please check the code ($\\mathtt{eval.py:~Line~163-191}$). In this homework, we use the Pascal VOC 2007 metric. **[10 points]** \n",
    "    2. Explain how SSD can be much faster compared to Faster RCNN [4]? **[10 points]**\n",
    "    3. Usually the number of negative bounding boxes (boxes without any object) is much larger than the number of positive bounding boxes. Explain how this imbalance is handled in SSD and Faster RCNN, respectively. **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emXaFowQTmx5"
   },
   "source": [
    "``Answer Q2.2.A here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLQKP7fTmx6"
   },
   "source": [
    "``Answer Q2.2.B here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYYNVBbqTmx7"
   },
   "source": [
    "``Answer Q2.2.C here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_mUY0leTmx8"
   },
   "source": [
    "3. Randomly pick up some images from the PASCAL VOC 2012 dataset and some from other sources. Visualize the bounding box prediction results and include a figure in your report. You can use the code in folder $\\mathtt{demo}$ for visualization. **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHKq10liTmx9"
   },
   "source": [
    "``Answer Q2.3 here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVtWXaJJTmx-"
   },
   "source": [
    "## References\n",
    "1. Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\" arXiv preprint arXiv:1511.07122 (2015).\n",
    "2. Zhao, Hengshuang, et al. \"Pyramid scene parsing network.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n",
    "3. Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016.\n",
    "4.  Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems. 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rCto9EuGTmx_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "hw3_questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
